{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import HelicobacterClassifier\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/p9vc6dt51918j4q47v387x880000gn/T/ipykernel_48303/3108494176.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for n in range(5):\n",
    "    model = HelicobacterClassifier()\n",
    "\n",
    "    state_dict = torch.load(\n",
    "        rf\"HelicobacterClassifier_fold{n}.pth\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    globals()[f'model_fold_{n}'] = model\n",
    "\n",
    "# Load models into a list\n",
    "models = [globals()[f'model_fold_{n}'] for n in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     CODI  DENSITAT\n",
      "0  B22-01     BAIXA\n",
      "1  B22-02     BAIXA\n",
      "2  B22-03  NEGATIVA\n",
      "3  B22-04  NEGATIVA\n",
      "4  B22-05  NEGATIVA\n",
      "    Pat_ID  Section_ID  Window_ID      i      j    h    w  Presence\n",
      "0  B22-129           0        659   7477  11978  256  256        -1\n",
      "1   B22-68           0        131   6597  12009  256  256        -1\n",
      "2   B22-68           0        141   5100  10737  256  256        -1\n",
      "3   B22-68           0        290   5015  14908  256  256        -1\n",
      "4   B22-68           0        298  11626  13928  256  256        -1\n",
      "        CODI  DENSITAT\n",
      "2     B22-03  NEGATIVA\n",
      "3     B22-04  NEGATIVA\n",
      "4     B22-05  NEGATIVA\n",
      "5     B22-06  NEGATIVA\n",
      "6     B22-07  NEGATIVA\n",
      "..       ...       ...\n",
      "304  B22-311      ALTA\n",
      "305  B22-312      ALTA\n",
      "306  B22-313      ALTA\n",
      "307  B22-314  NEGATIVA\n",
      "308  B22-315  NEGATIVA\n",
      "\n",
      "[237 rows x 2 columns]\n",
      "['NEGATIVA']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "path_to_annotated = r'HP_WSI-CoordAnnotatedPatches.xlsx'\n",
    "path_to_Patient_Diagnois = r'PatientDiagnosis.csv'\n",
    "patient_diagnosisDF = pd.read_csv(path_to_Patient_Diagnois)\n",
    "annotated_patchesDF = pd.read_excel(path_to_annotated) \n",
    "\n",
    "print(patient_diagnosisDF.head())\n",
    "print(annotated_patchesDF.head())\n",
    "\n",
    "patient_diagnosisDF = patient_diagnosisDF[(patient_diagnosisDF['DENSITAT'] == 'ALTA') | (patient_diagnosisDF['DENSITAT'] == 'NEGATIVA')]\n",
    "annotated_patchesDF = annotated_patchesDF[annotated_patchesDF['Presence'] != 0]\n",
    "\n",
    "\n",
    "# Group by patient_id and count the number of positive and negative patches\n",
    "grouped = annotated_patchesDF.groupby(['Pat_ID'])\n",
    "grouped = annotated_patchesDF.groupby('Pat_ID').agg(\n",
    "    number_of_positive_patches=('Presence', lambda x: (x == 1).sum()),\n",
    "    number_of_negative_patches=('Presence', lambda x: (x == -1).sum())\n",
    ").reset_index()\n",
    "\n",
    "# Inlcude in gropued pateint_diagnosisDF['DENSITAT'] based on the id\n",
    "grouped.head()\n",
    "grouped = grouped.merge(patient_diagnosisDF, left_on='Pat_ID', right_on='CODI', how='inner')\n",
    "grouped.head()\n",
    "grouped = grouped.drop(columns=['CODI'])\n",
    "\n",
    "# OBJECTIVE  have a dataframe with the following columns: patient_id, number_of_positive_patches, number_of_negative_patches, diagnosis, prediction\n",
    "print(patient_diagnosisDF)\n",
    "\n",
    "densitat = patient_diagnosisDF [patient_diagnosisDF['CODI'] == \"B22-03\"]\n",
    "print(list(densitat['DENSITAT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B22-161',\n",
       " 'B22-88',\n",
       " 'B22-04',\n",
       " 'B22-82',\n",
       " 'B22-286',\n",
       " 'B22-44',\n",
       " 'B22-65',\n",
       " 'B22-261',\n",
       " 'B22-225',\n",
       " 'B22-100',\n",
       " 'B22-282',\n",
       " 'B22-314',\n",
       " 'B22-271',\n",
       " 'B22-75',\n",
       " 'B22-196',\n",
       " 'B22-135',\n",
       " 'B22-310',\n",
       " 'B22-14',\n",
       " 'B22-231',\n",
       " 'B22-31',\n",
       " 'B22-10',\n",
       " 'B22-198',\n",
       " 'B22-96',\n",
       " 'B22-207',\n",
       " 'B22-226',\n",
       " 'B22-247',\n",
       " 'B22-266',\n",
       " 'B22-85',\n",
       " 'B22-128',\n",
       " 'B22-222',\n",
       " 'B22-203',\n",
       " 'B22-81',\n",
       " 'B22-66',\n",
       " 'B22-281',\n",
       " 'B22-62',\n",
       " 'B22-09',\n",
       " 'B22-03',\n",
       " 'B22-49',\n",
       " 'B22-309',\n",
       " 'B22-209',\n",
       " 'B22-262',\n",
       " 'B22-243',\n",
       " 'B22-285',\n",
       " 'B22-268',\n",
       " 'B22-07',\n",
       " 'B22-236',\n",
       " 'B22-78',\n",
       " 'B22-238',\n",
       " 'B22-272',\n",
       " 'B22-295',\n",
       " 'B22-132',\n",
       " 'B22-36',\n",
       " 'B22-213',\n",
       " 'B22-13',\n",
       " 'B22-32',\n",
       " 'B22-72',\n",
       " 'B22-257',\n",
       " 'B22-19',\n",
       " 'B22-136',\n",
       " 'B22-259',\n",
       " 'B22-17',\n",
       " 'B22-159',\n",
       " 'B22-246',\n",
       " 'B22-48',\n",
       " 'B22-206',\n",
       " 'B22-263',\n",
       " 'B22-229',\n",
       " 'B22-69',\n",
       " 'B22-227',\n",
       " 'B22-02',\n",
       " 'B22-146',\n",
       " 'B22-08',\n",
       " 'B22-267',\n",
       " 'B22-06',\n",
       " 'B22-202',\n",
       " 'B22-269',\n",
       " 'B22-169',\n",
       " 'B22-242',\n",
       " 'B22-208',\n",
       " 'B22-237',\n",
       " 'B22-12',\n",
       " 'B22-73',\n",
       " 'B22-212',\n",
       " 'B22-233',\n",
       " 'B22-294',\n",
       " 'B22-252',\n",
       " 'B22-239',\n",
       " 'B22-18',\n",
       " 'B22-39',\n",
       " 'B22-58',\n",
       " 'B22-79',\n",
       " 'B22-273',\n",
       " 'B22-16',\n",
       " 'B22-139',\n",
       " 'B22-05',\n",
       " 'B22-220',\n",
       " 'B22-105',\n",
       " 'B22-41',\n",
       " 'B22-224',\n",
       " 'B22-89',\n",
       " 'B22-201',\n",
       " 'B22-120',\n",
       " 'B22-01',\n",
       " 'B22-20',\n",
       " 'B22-205',\n",
       " 'B22-283',\n",
       " 'B22-211',\n",
       " 'B22-130',\n",
       " 'B22-315',\n",
       " 'B22-134',\n",
       " 'B22-293',\n",
       " 'B22-15',\n",
       " 'B22-255',\n",
       " 'B22-199',\n",
       " 'B22-215',\n",
       " 'B22-11']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib \n",
    "grouped\n",
    "\n",
    "# Create a pathlib object to store the results\n",
    "\n",
    "path_to_holdout =  r\"HoldOut\"\n",
    "# iterate over the directories in holdout  and include the name of the directory in a list\n",
    "holdout_directories = [x for x in pathlib.Path(path_to_holdout).iterdir() if x.is_dir()]\n",
    "\n",
    "# Extract the name of the directories without the full path\n",
    "holdout_directories = [x.name[:-2] for x in holdout_directories]\n",
    "\n",
    "holdout_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def transform_image(image, size):\n",
    "    return image.resize(size)\n",
    "\n",
    "class HoldoutDataset():\n",
    "    def __init__(self):\n",
    "        path_to_holdout = r\"HoldOut\"\n",
    "        patient_directories = [patient for patient in pathlib.Path(path_to_holdout).iterdir()]\n",
    "        path_to_Patient_Diagnois = r'PatientDiagnosis.csv'\n",
    "        patient_diagnosisDF = pd.read_csv(path_to_Patient_Diagnois)\n",
    "        patient_diagnosisDF = patient_diagnosisDF[(patient_diagnosisDF['DENSITAT'] == 'ALTA') | (patient_diagnosisDF['DENSITAT'] == 'NEGATIVA')]\n",
    "        patient_diagnosisDF['DENSITAT'] = [1 if x == 'ALTA' else -1 for x in patient_diagnosisDF['DENSITAT']]\n",
    "        patient_diagnosisDF = patient_diagnosisDF.rename(columns={'DENSITAT': 'DiagnosisGT'})\n",
    "        \n",
    "        self.dictionary = {}\n",
    "        \n",
    "        for patient in patient_directories:\n",
    "            if patient.name[:-2] in patient_diagnosisDF['CODI'].values:\n",
    "                images = [x for x in patient.iterdir() if x.is_file() and x.name.endswith('.png')]\n",
    "                self.dictionary[patient.name[:-2]] = (images, patient_diagnosisDF[patient_diagnosisDF['CODI'] == patient.name[:-2]]['DiagnosisGT'].values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dictionary)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = list(self.dictionary.keys())[idx]\n",
    "        images, patient_diagnosis = self.dictionary[patient_id]\n",
    "        transformed_images = [transform_image(Image.open(image_path).convert(\"RGB\"), (256, 256)) for image_path in images]\n",
    "        return transformed_images, patient_id, patient_diagnosis\n",
    "\n",
    "\n",
    "holdout_dataset = HoldoutDataset() \n",
    "print(len(holdout_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7s/p9vc6dt51918j4q47v387x880000gn/T/ipykernel_48303/3515831914.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(rf\"HelicobacterClassifier_fold{n}.pth\",\n",
      "Processing patients:   1%|          | 1/82 [01:05<1:28:50, 65.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient ID: B22-88, Predicted Diagnosis: -1, Ground Truth: -1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Define an Attention Mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_weights = nn.Linear(input_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Tensor of shape (num_patches, feature_dim)\n",
    "        Returns:\n",
    "            weighted_output: Tensor of shape (feature_dim,)\n",
    "        \"\"\"\n",
    "        scores = self.attention_weights(features)  # Shape: (num_patches, 1)\n",
    "        scores = torch.softmax(scores, dim=0)  # Normalize scores across patches\n",
    "        weighted_output = torch.sum(features * scores, dim=0)  # Weighted sum of features\n",
    "        return weighted_output, scores\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "models = []\n",
    "for n in range(5):\n",
    "    model = HelicobacterClassifier()\n",
    "    model.load_state_dict(torch.load(rf\"HelicobacterClassifier_fold{n}.pth\"))   \n",
    "    models.append(model.to(device))\n",
    "\n",
    "# Initialize Attention\n",
    "attention = Attention(input_dim=2).to(device)  # Assuming the output features are probabilities with 2 classes\n",
    "\n",
    "#--------------------------------------------------------------\n",
    "\n",
    "# Create a DataLoader for the holdout dataset\n",
    "holdout_loader = DataLoader(holdout_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "# Predict diagnosis for each patient using ensemble with attention\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "for data in tqdm(holdout_loader, desc=\"Processing patients\"):\n",
    "    data = data[0]  # Remove batch dimension\n",
    "    images, patient_id, patient_diagnosis = data\n",
    "    images = torch.stack([torchvision.transforms.ToTensor()(image) for image in images])\n",
    "    images = images.to(device)\n",
    "    \n",
    "    # Extract outputs from all models for all patches\n",
    "    patch_features = []\n",
    "    for img in images:\n",
    "        img = img.unsqueeze(0)  # Add batch dimension\n",
    "        outputs = [torch.softmax(model(img), dim=1) for model in models]  # Shape: (num_models, num_classes)\n",
    "        outputs = torch.mean(torch.stack(outputs), dim=0)  # Average over models\n",
    "        patch_features.append(outputs.squeeze(0))  # Shape: (num_classes,)\n",
    "    \n",
    "    patch_features = torch.stack(patch_features)  # Shape: (num_patches, num_classes)\n",
    "    \n",
    "    # Apply attention mechanism\n",
    "    weighted_output, scores = attention(patch_features)  # Shape: (num_classes,), (num_patches, 1)\n",
    "    _, predicted = torch.max(weighted_output, dim=0)  # Final prediction based on weighted output\n",
    "    \n",
    "    # Interpret diagnosis\n",
    "    final_prediction = 1 if predicted.item() == 1 else -1\n",
    "    predictions.append(final_prediction)\n",
    "    ground_truth.append(patient_diagnosis[0].item())\n",
    "\n",
    "    print(f\"Patient ID: {patient_id}, Predicted Diagnosis: {final_prediction}, Ground Truth: {patient_diagnosis[0].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(ground_truth, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Compute classification report\n",
    "class_report = classification_report(ground_truth, predictions, target_names=['Negative', 'Positive'])\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
