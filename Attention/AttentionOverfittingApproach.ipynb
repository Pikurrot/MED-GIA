{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLU_QYQu6ks7"
      },
      "source": [
        "### Experimental design: Attention\n",
        "Use of Attention mechanism as patient diagnosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUMy3hWW6ks8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class HelicobacterClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HelicobacterClassifier, self).__init__()\n",
        "\n",
        "        # conv1 + pooling (B, C, H, W) -> (B, 32, H/2, W/2)\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        # conv2 + pooling (B, 32, H/2, W/2) -> (B, 64, H/4, W/4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        # conv3 + pooling (B, 64, H/4, W/4) -> (B, 128, H/8, W/8)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(128 * 32 * 32, 512)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 32 * 32)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the Attention Module\n",
        "class AttentionModule(nn.Module):\n",
        "    def __init__(self, feature_dim, hidden_dim):\n",
        "        super(AttentionModule, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(feature_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)  # Output scalar attention score\n",
        "        )\n",
        "\n",
        "    def forward(self, patch_features):\n",
        "        # Compute attention scores\n",
        "        scores = self.attention(patch_features).squeeze(-1)  # Shape: [N]\n",
        "        weights = F.softmax(scores, dim=0)  # Normalize scores across patches\n",
        "\n",
        "        # Weighted sum of patch features\n",
        "        aggregated_features = torch.sum(weights.unsqueeze(-1) * patch_features, dim=0)\n",
        "        return aggregated_features, weights\n",
        "\n",
        "\n",
        "# Define the Patient-Level Classifier\n",
        "class PatientLevelClassifier(nn.Module):\n",
        "    def __init__(self, feature_dim, hidden_dim, num_classes):\n",
        "        super(PatientLevelClassifier, self).__init__()\n",
        "        self.attention = AttentionModule(feature_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "    def forward(self, patch_features):\n",
        "        # Attention-based aggregation\n",
        "        aggregated_features, weights = self.attention(patch_features)\n",
        "\n",
        "        # Patient-level classification\n",
        "        logits = self.fc(aggregated_features)\n",
        "        return logits, weights\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, patch_model_path, output_layer_dim):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        # Initialize the model architecture\n",
        "        self.patch_model = HelicobacterClassifier()\n",
        "\n",
        "        # Load the state dictionary\n",
        "        state_dict = torch.load(patch_model_path, map_location=torch.device('cpu'))\n",
        "        self.patch_model.load_state_dict(state_dict)\n",
        "\n",
        "        # Remove the final classification layer (fc2) to extract features\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            self.patch_model.conv1,\n",
        "            self.patch_model.pool,\n",
        "            self.patch_model.conv2,\n",
        "            self.patch_model.pool,\n",
        "            self.patch_model.conv3,\n",
        "            self.patch_model.pool,\n",
        "            nn.Flatten(),\n",
        "            self.patch_model.fc1  # Use fc1 as the final feature layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features\n",
        "        with torch.no_grad():\n",
        "            features = self.feature_extractor(x)\n",
        "        return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6MSGfp-6ks9",
        "outputId": "a50e15b2-8c89-4ec3-a9ed-bd3ddd7bae41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     CODI  DENSITAT\n",
            "0  B22-01     BAIXA\n",
            "1  B22-02     BAIXA\n",
            "2  B22-03  NEGATIVA\n",
            "3  B22-04  NEGATIVA\n",
            "4  B22-05  NEGATIVA\n",
            "    Pat_ID  Section_ID  Window_ID      i      j    h    w  Presence\n",
            "0  B22-129           0        659   7477  11978  256  256        -1\n",
            "1   B22-68           0        131   6597  12009  256  256        -1\n",
            "2   B22-68           0        141   5100  10737  256  256        -1\n",
            "3   B22-68           0        290   5015  14908  256  256        -1\n",
            "4   B22-68           0        298  11626  13928  256  256        -1\n",
            "        CODI  DENSITAT\n",
            "2     B22-03  NEGATIVA\n",
            "3     B22-04  NEGATIVA\n",
            "4     B22-05  NEGATIVA\n",
            "5     B22-06  NEGATIVA\n",
            "6     B22-07  NEGATIVA\n",
            "..       ...       ...\n",
            "304  B22-311      ALTA\n",
            "305  B22-312      ALTA\n",
            "306  B22-313      ALTA\n",
            "307  B22-314  NEGATIVA\n",
            "308  B22-315  NEGATIVA\n",
            "\n",
            "[237 rows x 2 columns]\n",
            "['NEGATIVA']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/7s/p9vc6dt51918j4q47v387x880000gn/T/ipykernel_98858/2430084478.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(patch_model_path, map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "82\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['B22-161',\n",
              " 'B22-88',\n",
              " 'B22-04',\n",
              " 'B22-82',\n",
              " 'B22-286',\n",
              " 'B22-44',\n",
              " 'B22-65',\n",
              " 'B22-261',\n",
              " 'B22-225',\n",
              " 'B22-100',\n",
              " 'B22-282',\n",
              " 'B22-314',\n",
              " 'B22-271',\n",
              " 'B22-75',\n",
              " 'B22-196',\n",
              " 'B22-135',\n",
              " 'B22-310',\n",
              " 'B22-14',\n",
              " 'B22-231',\n",
              " 'B22-31',\n",
              " 'B22-10',\n",
              " 'B22-198',\n",
              " 'B22-96',\n",
              " 'B22-207',\n",
              " 'B22-226',\n",
              " 'B22-247',\n",
              " 'B22-266',\n",
              " 'B22-85',\n",
              " 'B22-128',\n",
              " 'B22-222',\n",
              " 'B22-203',\n",
              " 'B22-81',\n",
              " 'B22-66',\n",
              " 'B22-281',\n",
              " 'B22-62',\n",
              " 'B22-09',\n",
              " 'B22-03',\n",
              " 'B22-49',\n",
              " 'B22-309',\n",
              " 'B22-209',\n",
              " 'B22-262',\n",
              " 'B22-243',\n",
              " 'B22-285',\n",
              " 'B22-268',\n",
              " 'B22-07',\n",
              " 'B22-236',\n",
              " 'B22-78',\n",
              " 'B22-238',\n",
              " 'B22-272',\n",
              " 'B22-295',\n",
              " 'B22-132',\n",
              " 'B22-36',\n",
              " 'B22-213',\n",
              " 'B22-13',\n",
              " 'B22-32',\n",
              " 'B22-72',\n",
              " 'B22-257',\n",
              " 'B22-19',\n",
              " 'B22-136',\n",
              " 'B22-259',\n",
              " 'B22-17',\n",
              " 'B22-159',\n",
              " 'B22-246',\n",
              " 'B22-48',\n",
              " 'B22-206',\n",
              " 'B22-263',\n",
              " 'B22-229',\n",
              " 'B22-69',\n",
              " 'B22-227',\n",
              " 'B22-02',\n",
              " 'B22-146',\n",
              " 'B22-08',\n",
              " 'B22-267',\n",
              " 'B22-06',\n",
              " 'B22-202',\n",
              " 'B22-269',\n",
              " 'B22-169',\n",
              " 'B22-242',\n",
              " 'B22-208',\n",
              " 'B22-237',\n",
              " 'B22-12',\n",
              " 'B22-73',\n",
              " 'B22-212',\n",
              " 'B22-233',\n",
              " 'B22-294',\n",
              " 'B22-252',\n",
              " 'B22-239',\n",
              " 'B22-18',\n",
              " 'B22-39',\n",
              " 'B22-58',\n",
              " 'B22-79',\n",
              " 'B22-273',\n",
              " 'B22-16',\n",
              " 'B22-139',\n",
              " 'B22-05',\n",
              " 'B22-220',\n",
              " 'B22-105',\n",
              " 'B22-41',\n",
              " 'B22-224',\n",
              " 'B22-89',\n",
              " 'B22-201',\n",
              " 'B22-120',\n",
              " 'B22-01',\n",
              " 'B22-20',\n",
              " 'B22-205',\n",
              " 'B22-283',\n",
              " 'B22-211',\n",
              " 'B22-130',\n",
              " 'B22-315',\n",
              " 'B22-134',\n",
              " 'B22-293',\n",
              " 'B22-15',\n",
              " 'B22-255',\n",
              " 'B22-199',\n",
              " 'B22-215',\n",
              " 'B22-11']"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "path_to_annotated = '/Users/marino/Documents/GitHub/MED-GIA/MED-GIA/HP_WSI-CoordAnnotatedPatches.xlsx'\n",
        "path_to_Patient_Diagnois = '/Users/marino/Documents/GitHub/MED-GIA/MED-GIA/PatientDiagnosis.csv'\n",
        "patient_diagnosisDF = pd.read_csv(path_to_Patient_Diagnois)\n",
        "annotated_patchesDF = pd.read_excel(path_to_annotated)\n",
        "\n",
        "print(patient_diagnosisDF.head())\n",
        "print(annotated_patchesDF.head())\n",
        "\n",
        "patient_diagnosisDF = patient_diagnosisDF[(patient_diagnosisDF['DENSITAT'] == 'ALTA') | (patient_diagnosisDF['DENSITAT'] == 'NEGATIVA')]\n",
        "annotated_patchesDF = annotated_patchesDF[annotated_patchesDF['Presence'] != 0]\n",
        "\n",
        "\n",
        "# Group by patient_id and count the number of positive and negative patches\n",
        "grouped = annotated_patchesDF.groupby(['Pat_ID'])\n",
        "grouped = annotated_patchesDF.groupby('Pat_ID').agg(\n",
        "    number_of_positive_patches=('Presence', lambda x: (x == 1).sum()),\n",
        "    number_of_negative_patches=('Presence', lambda x: (x == -1).sum())\n",
        ").reset_index()\n",
        "\n",
        "# Inlcude in gropued pateint_diagnosisDF['DENSITAT'] based on the id\n",
        "grouped.head()\n",
        "grouped = grouped.merge(patient_diagnosisDF, left_on='Pat_ID', right_on='CODI', how='inner')\n",
        "grouped.head()\n",
        "grouped = grouped.drop(columns=['CODI'])\n",
        "\n",
        "# OBJECTIVE  have a dataframe with the following columns: patient_id, number_of_positive_patches, number_of_negative_patches, diagnosis, prediction\n",
        "\n",
        "print(patient_diagnosisDF)\n",
        "\n",
        "densitat = patient_diagnosisDF [patient_diagnosisDF['CODI'] == \"B22-03\"]\n",
        "print(list(densitat['DENSITAT']))\n",
        "\n",
        "\n",
        "import pathlib\n",
        "grouped\n",
        "\n",
        "\n",
        "# Create a pathlib object to store the results\n",
        "\n",
        "path_to_holdout =  'HoldOut'\n",
        "# iterate over the directories in holdout  and include the name of the directory in a list\n",
        "holdout_directories = [x for x in pathlib.Path(path_to_holdout).iterdir() if x.is_dir()]\n",
        "\n",
        "# Extract the name of the directories without the full path\n",
        "holdout_directories = [x.name[:-2] for x in holdout_directories]\n",
        "\n",
        "from PIL import Image\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def transform_image(image, size):\n",
        "    return image.resize(size)\n",
        "\n",
        "class HoldoutDataset():\n",
        "    def __init__(self, patch_model_path, output_layer_dim, device=\"cpu\"):\n",
        "        self.device = device\n",
        "        self.patch_model = FeatureExtractor(patch_model_path, output_layer_dim).to(device)\n",
        "\n",
        "        # Dataset details\n",
        "        self.path_to_holdout = 'HoldOut'\n",
        "        self.patient_directories = [patient for patient in pathlib.Path(self.path_to_holdout).iterdir()]\n",
        "        self.path_to_Patient_Diagnois = '/Users/marino/Documents/GitHub/MED-GIA/MED-GIA/PatientDiagnosis.csv'\n",
        "        self.patient_diagnosisDF = pd.read_csv(self.path_to_Patient_Diagnois)\n",
        "        self.patient_diagnosisDF = self.patient_diagnosisDF[(self.patient_diagnosisDF['DENSITAT'] == 'ALTA') |\n",
        "                                                            (self.patient_diagnosisDF['DENSITAT'] == 'NEGATIVA')]\n",
        "        self.patient_diagnosisDF['DENSITAT'] = [1 if x == 'ALTA' else -1 for x in self.patient_diagnosisDF['DENSITAT']]\n",
        "        self.patient_diagnosisDF = self.patient_diagnosisDF.rename(columns={'DENSITAT': 'DiagnosisGT'})\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        self.dictionary = {}\n",
        "        for patient in self.patient_directories:\n",
        "            patient_id = patient.name[:-2]\n",
        "            if patient_id in self.patient_diagnosisDF['CODI'].values:\n",
        "                images = [x for x in patient.iterdir() if x.is_file() and x.name.endswith('.png')]\n",
        "                diagnosis = self.patient_diagnosisDF[self.patient_diagnosisDF['CODI'] == patient_id]['DiagnosisGT'].values[0]\n",
        "                self.dictionary[patient_id] = (images, diagnosis)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dictionary)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient_id = list(self.dictionary.keys())[idx]\n",
        "        images, diagnosis = self.dictionary[patient_id]\n",
        "\n",
        "        patch_features = []\n",
        "        for image_path in images:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            image = self.transform(image).unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                feature = self.patch_model(image)\n",
        "            patch_features.append(feature.cpu().squeeze(0))\n",
        "\n",
        "        patch_features = torch.stack(patch_features)  # Shape: [N, D]\n",
        "        return patch_features, patient_id, diagnosis\n",
        "\n",
        "\n",
        "# Required paths and parameters\n",
        "patch_model_path = 'classifier.pth'  # Path to your trained patch classification model\n",
        "output_layer_dim = 512  # The feature dimension output by your patch model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate the HoldoutDataset\n",
        "holdout_dataset = HoldoutDataset(patch_model_path=patch_model_path,\n",
        "                                 output_layer_dim=output_layer_dim,\n",
        "                                 device=device)\n",
        "print(len(holdout_dataset))\n",
        "\n",
        "holdout_directories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evwT5jCj6ks_"
      },
      "source": [
        "### Attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nba2BWL46ks_",
        "outputId": "1ca3aaeb-f8ae-47cc-aace-2bb1c8f71ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded with 82 patients.\n"
          ]
        }
      ],
      "source": [
        "# Update the feature dimension to match the output from fc1\n",
        "output_layer_dim = 512  # Matches fc1 output from FeatureExtractor\n",
        "hidden_dim = 128        # Hidden dimension in the attention mechanism\n",
        "num_classes = 2         # Binary classification (e.g., POSITIVE/NEGATIVE)\n",
        "\n",
        "\n",
        "# Initialize the patient-level classifier\n",
        "patient_classifier = PatientLevelClassifier(\n",
        "    feature_dim=output_layer_dim,  # Updated feature dimension\n",
        "    hidden_dim=hidden_dim,\n",
        "    num_classes=num_classes\n",
        ").to(device)\n",
        "\n",
        "\n",
        "print(f\"Dataset loaded with {len(holdout_dataset)} patients.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBsOpL6h6ks_"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apx4c6Sb6ks_",
        "outputId": "d60615af-6930-4cd3-c1a9-12d63bc1c7b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patient ID: B22-88, Ground Truth: -1, Prediction: 1\n",
            "Patient ID: B22-04, Ground Truth: -1, Prediction: 1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[63], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m patient_classifier\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Iterate over patients in the dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patch_features, patient_id, diagnosis_gt \u001b[38;5;129;01min\u001b[39;00m holdout_dataset:\n\u001b[1;32m      6\u001b[0m     patch_features \u001b[38;5;241m=\u001b[39m patch_features\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     diagnosis_gt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(diagnosis_gt)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Ensure GT labels are on the same device\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[61], line 99\u001b[0m, in \u001b[0;36mHoldoutDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     97\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 99\u001b[0m         feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     patch_features\u001b[38;5;241m.\u001b[39mappend(feature\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    102\u001b[0m patch_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(patch_features)  \u001b[38;5;66;03m# Shape: [N, D]\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[60], line 90\u001b[0m, in \u001b[0;36mFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 90\u001b[0m         features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
            "File \u001b[0;32m~/miniconda3/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/vision/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/vision/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/vision/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Set the patient classifier to evaluation mode\n",
        "patient_classifier.eval()\n",
        "\n",
        "# Iterate over patients in the dataset\n",
        "for patch_features, patient_id, diagnosis_gt in holdout_dataset:\n",
        "    patch_features = patch_features.to(device)\n",
        "    diagnosis_gt = torch.tensor(diagnosis_gt).to(device)  # Ensure GT labels are on the same device\n",
        "\n",
        "    # Predict patient-level diagnosis\n",
        "    with torch.no_grad():\n",
        "        logits, attention_weights = patient_classifier(patch_features)\n",
        "\n",
        "    diagnosis_pred = logits.argmax(dim=-1).item()\n",
        "    print(f\"Patient ID: {patient_id}, Ground Truth: {diagnosis_gt}, Prediction: {diagnosis_pred}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vision",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}