{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2676\n",
      "torch.Size([3, 256, 256]) 0\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "default_path = \"/fhome/vlia/HelicoDataSet\"\n",
    "config_path = \"config.yml\"\n",
    "\n",
    "def ensure_dataset_path_yaml() -> int:\n",
    "\t\"\"\"\n",
    "\tEnsures that config.yml contains the dataset path.\n",
    "\t:return: 0 if path is valid, 1 if path is not present, 2 if path is invalid\n",
    "\t\"\"\"\n",
    "\t# Check config.yml exists\n",
    "\tconfig = {}\n",
    "\n",
    "\t# Check if config.yml exists\n",
    "\tif not os.path.exists(config_path):\n",
    "\t\tconfig[\"dataset_path\"] = default_path\n",
    "\t\twith open(config_path, \"w\") as file:\n",
    "\t\t\tyaml.safe_dump(config, file)\n",
    "\t\treturn 1\n",
    "\n",
    "\t# Load existing config\n",
    "\ttry:\n",
    "\t\twith open(config_path, \"r\") as file:\n",
    "\t\t\tconfig = yaml.safe_load(file) or {}\n",
    "\texcept yaml.YAMLError as e:\n",
    "\t\traise ValueError(f\"Error parsing {config_path}: {e}\")\n",
    "\n",
    "\t# Check if 'dataset_path' exists in config\n",
    "\tif \"dataset_path\" not in config:\n",
    "\t\tconfig[\"dataset_path\"] = default_path\n",
    "\t\twith open(config_path, \"w\") as file:\n",
    "\t\t\tyaml.safe_dump(config, file)\n",
    "\t\treturn 1\n",
    "\n",
    "\t# Check if the dataset path exists on the filesystem\n",
    "\tif not os.path.exists(config[\"dataset_path\"]):\n",
    "\t\treturn 2\n",
    "\n",
    "\treturn 0\n",
    "\n",
    "def listdir(path: str, filter: str = None, extension: str = None) -> list:\n",
    "\t\"\"\"\n",
    "\tReturns a list of directories in the given path.\n",
    "\tIf a filter is provided, only directories that contain the filter in their name will be returned.\n",
    "\n",
    "\t:param path: Path to the directory\n",
    "\t:param filter: Filter to apply to the directories\n",
    "\t:return: List of directory names as strings\n",
    "\t\"\"\"\n",
    "\tdirectories = os.listdir(path)\n",
    "\tif filter is not None:\n",
    "\t\tdirectories = [directory for directory in directories if filter in directory]\n",
    "\tif extension is not None:\n",
    "\t\tdirectories = [directory for directory in directories if directory.endswith(extension)]\n",
    "\treturn directories\n",
    "\n",
    "def transform_image(image: Image, size: tuple) -> Image:\n",
    "\ttransformations = transforms.Compose([\n",
    "\t\ttransforms.Resize(size),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\t])\n",
    "\treturn transformations(image)\n",
    "\n",
    "class HelicoDatasetAnomalyDetection(Dataset):\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\t# Initialize paths\n",
    "\t\tpath_error = ensure_dataset_path_yaml()\n",
    "\t\tif path_error == 1:\n",
    "\t\t\tprint(f\"Dataset path not found in config.yml. Defaulting to {default_path}\")\n",
    "\t\t\tif not os.path.exists(default_path):\n",
    "\t\t\t\traise FileNotFoundError(f\"Default path {default_path} does not exist. Specify a valid path in config.yml.\")\n",
    "\t\telif path_error == 2:\n",
    "\t\t\tcurrent_path = yaml.safe_load(open(\"config.yml\", \"r\"))[\"dataset_path\"]\n",
    "\t\t\traise FileNotFoundError(f\"Dataset path {current_path} does not exist. Specify a valid path in config.yml.\")\n",
    "\t\t\n",
    "\t\tself.dataset_path = yaml.safe_load(open(\"config.yml\", \"r\"))[\"dataset_path\"]\n",
    "\t\tself.csv_file_path = os.path.join(self.dataset_path, \"PatientDiagnosis.csv\")\n",
    "\t\tself.cropped_path = os.path.join(self.dataset_path, \"CrossValidation\", \"Cropped\")\n",
    "\n",
    "\t\t# Find all the negative diagnosis directories\n",
    "\t\tpaths_negatives = self.get_negative_diagnosis_directories(self.csv_file_path)\n",
    "\t\tpaths = [os.path.join(self.cropped_path, filename) for filename in listdir(self.cropped_path)]\n",
    "\t\tactual_paths = []\n",
    "\t\tfor path_negative in paths_negatives:\n",
    "\t\t\tfor path in paths:\n",
    "\t\t\t\tif path_negative == path[:-2]:\n",
    "\t\t\t\t\tactual_paths.append(path)\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t# Retrieve all the patches from the directories\n",
    "\t\tself.paths_patches = []\n",
    "\t\tfor directory in actual_paths:\n",
    "\t\t\tpatches_names = listdir(directory, extension=\".png\")\n",
    "\t\t\tpatches_paths = [os.path.join(directory, patches_name) for patches_name in patches_names]\n",
    "\t\t\tself.paths_patches.extend(patches_paths)\n",
    "\n",
    "\tdef get_negative_diagnosis_directories(self, csv_path: str) -> list:\n",
    "\t\t\"\"\"\n",
    "\t\tGiven a CSV file path, returns a list of directories for the NEGATIVE Diagnosis.\n",
    "\t\tEach directory follows the format \"/fhome/vlia/helicoDataSet/CrossValidation/Cropped/patientCODI\",\n",
    "\t\twhere \"patientCODI\" is based on the CODI column in the CSV.\n",
    "\n",
    "\t\t:param csv_path: Path to the CSV file\n",
    "\t\t:return: List of directory paths as strings\n",
    "\t\t\"\"\"\n",
    "\t\tdata = pd.read_csv(csv_path)\n",
    "\n",
    "\t\t# Filter rows where the DENSITAT is \"NEGATIVA\"\n",
    "\t\tnegative_diagnosis = data[data[\"DENSITAT\"] == \"NEGATIVA\"]\n",
    "\n",
    "\t\t# Create directory paths based on the CODI values\n",
    "\t\tdirectories = [\n",
    "\t\t\tos.path.join(self.cropped_path, codi)\n",
    "\t\t\tfor codi in negative_diagnosis[\"CODI\"]\n",
    "\t\t]\n",
    "\n",
    "\t\treturn directories\n",
    "\n",
    "\tdef __getitem__(self, index) -> Any:\n",
    "\t\treturn transform_image(Image.open(self.paths_patches[index]).convert(\"RGB\"), (256, 256))\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.paths_patches)\n",
    "\n",
    "\n",
    "class HelicoDatasetClassification(Dataset):\n",
    "\tdef __init__(self) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\t# Initialize paths\n",
    "\t\tpath_error = ensure_dataset_path_yaml()\n",
    "\t\tif path_error == 1:\n",
    "\t\t\tprint(f\"Dataset path not found in config.yml. Defaulting to {default_path}\")\n",
    "\t\t\tif not os.path.exists(default_path):\n",
    "\t\t\t\traise FileNotFoundError(f\"Default path {default_path} does not exist. Specify a valid path in config.yml.\")\n",
    "\t\telif path_error == 2:\n",
    "\t\t\tcurrent_path = yaml.safe_load(open(\"config.yml\", \"r\"))[\"dataset_path\"]\n",
    "\t\t\traise FileNotFoundError(f\"Dataset path {current_path} does not exist. Specify a valid path in config.yml.\")\n",
    "\t\t\n",
    "\t\tself.dataset_path = yaml.safe_load(open(\"config.yml\", \"r\"))[\"dataset_path\"]\n",
    "\t\tself.annotated_path = os.path.join(self.dataset_path, \"CrossValidation\", \"Annotated\")\n",
    "\t\tself.excel_file_path = os.path.join(self.dataset_path, \"HP_WSI-CoordAnnotatedPatches.xlsx\")\n",
    "\n",
    "\t\tself.paths_labels = self.get_paths_and_labels(self.annotated_path, self.excel_file_path)\n",
    "\t\n",
    "\tdef get_paths_and_labels(self, annotated_path: str, excel_path: str) -> tuple:\n",
    "\t\t\"\"\"\n",
    "\t\tGiven the annotated path and an Excel file path with columns \"Path_ID\", \"Window_ID\", and \"Presence\" (which ranges -1, 0, 1),\n",
    "\t\treturns a list of tuples (path, label). The label is 0 if the presence is -1, and 1 if the presence is 1.\n",
    "\t\tSamples with \"Presence\" 0 are ignored.\n",
    "\t\t\"\"\"\n",
    "\t\t# Load the Excel file\n",
    "\t\tdata = pd.read_excel(excel_path)\n",
    "\n",
    "\t\t# Filter the rows with \"Presence\" -1 or 1\n",
    "\t\tdata = data[(data[\"Presence\"] == -1) | (data[\"Presence\"] == 1)]\n",
    "\n",
    "\t\t# Create a list of tuples (path, label)\n",
    "\t\tpaths_labels = [\n",
    "\t\t\t(os.path.join(annotated_path, f\"{row['Pat_ID']}\",f\"{row['Window_ID']}.png\"), 0 if row[\"Presence\"] == -1 else 1)\n",
    "\t\t\tfor _, row in data.iterrows()\n",
    "\t\t]\n",
    "\t\tpaths = [os.path.join(annotated_path, filename) for filename in listdir(annotated_path)]\n",
    "\t\tactual_paths = []\n",
    "\t\tfor path_label_tup in paths_labels:\n",
    "\t\t\tpath_label = os.path.dirname(path_label_tup[0])\n",
    "\t\t\tfor path in paths:\n",
    "\t\t\t\tif path_label == path[:-2]:\n",
    "\t\t\t\t\tbasename = os.path.splitext(os.path.basename(path_label_tup[0]))[0].zfill(5)\n",
    "\t\t\t\t\tlisted_basenames = listdir(path)\n",
    "\t\t\t\t\tactual_basenames = [listed_basename for listed_basename in listed_basenames if listed_basename.startswith(basename)]\n",
    "\t\t\t\t\tlabel_append = path_label_tup[1]\n",
    "\t\t\t\t\tfor actual_basename in actual_basenames:\n",
    "\t\t\t\t\t\tpath_append = os.path.join(path, actual_basename)\n",
    "\t\t\t\t\t\tactual_paths.append((path_append, label_append))\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\treturn actual_paths\n",
    "\t\t\n",
    "\tdef __getitem__(self, index) -> Any:\n",
    "\t\tpath, label = self.paths_labels[index]\n",
    "\t\treturn transform_image(Image.open(path).convert(\"RGB\"), (256, 256)), label\n",
    "\t\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.paths_labels)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t# dataset = HelicoDatasetAnomalyDetection()\n",
    "\tdataset = HelicoDatasetClassification()\n",
    "\tprint(len(dataset))\n",
    "\tprint(dataset[0][0].shape, dataset[0][1])\n",
    "\t# Split the dataset into training and testing sets\n",
    "\ttrain_size = int(0.8 * len(dataset))\n",
    "\ttest_size = len(dataset) - train_size\n",
    "\ttrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\tk_folds = 5\n",
    "\n",
    "\t# Extract labels for StratifiedKFold\n",
    "\tlabels = [label for _, label in train_dataset]\n",
    "\n",
    "\tstratified_kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\tfor fold, (train_idx, val_idx) in enumerate(stratified_kfold.split(train_dataset, labels)):\n",
    "\t\tprint(f\"FOLD {fold}\")\n",
    "\t\tprint(\"--------------------------------\")\n",
    "\n",
    "\t\t# Sample elements randomly from a given list of indices, no replacement.\n",
    "\t\ttrain_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "\t\tval_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path exists: C:\\Users\\Usuario\\Documents\\GitHub\\MED-GIA\\data\\HelicoDataSet\\HP_WSI-CoordAnnotatedPatches.xlsx\n",
      "The paths are different:\n",
      "Path to check: C:\\Users\\Usuario\\Documents\\GitHub\\MED-GIA\\data\\HelicoDataSet\\HP_WSI-CoordAnnotatedPatches.xlsx\n",
      "Ground truth: C:\\Users\\Usuario\\Documents\\GitHub\\MED-GIA\\data\\HelicoDataSet\\HP_WSI-CoordAllAnnotatedPatches.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to check\n",
    "path_to_check = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\GitHub\\\\MED-GIA\\\\data\\\\HelicoDataSet\\\\HP_WSI-CoordAnnotatedPatches.xlsx\"\n",
    "\n",
    "# Check if the path exists\n",
    "if os.path.exists(path_to_check):\n",
    "    print(f\"The path exists: {path_to_check}\")\n",
    "else:\n",
    "    print(f\"The path does not exist: {path_to_check}\")\n",
    "\n",
    "gt = \"C:\\\\Users\\\\Usuario\\\\Documents\\\\GitHub\\\\MED-GIA\\\\data\\\\HelicoDataSet\\\\HP_WSI-CoordAllAnnotatedPatches.xlsx\"\n",
    "\n",
    "if path_to_check != gt:\n",
    "    print(f\"The paths are different:\\nPath to check: {path_to_check}\\nGround truth: {gt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HelicobacterClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HelicobacterClassifier, self).__init__()\n",
    "\n",
    "        # (B, C, H, W) -> (B, 32, H/2, W/2)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        # (B, 32, H/2, W/2) -> (B, 64, H/4, W/4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        # (B, 64, H/4, W/4) -> (B, 128, H/8, W/8)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        # (B, 128, H/8, W/8) -> (B, 128, H/16, W/16)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 32 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zoacu2kr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-sky-26</strong> at: <a href='https://wandb.ai/uab-ai/MED-GIA/runs/zoacu2kr' target=\"_blank\">https://wandb.ai/uab-ai/MED-GIA/runs/zoacu2kr</a><br/> View project at: <a href='https://wandb.ai/uab-ai/MED-GIA' target=\"_blank\">https://wandb.ai/uab-ai/MED-GIA</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241109_155125-zoacu2kr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zoacu2kr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Usuario\\Documents\\GitHub\\MED-GIA\\Classfication\\wandb\\run-20241109_155213-czvq7hz4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uab-ai/MED-GIA/runs/czvq7hz4' target=\"_blank\">sage-night-27</a></strong> to <a href='https://wandb.ai/uab-ai/MED-GIA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uab-ai/MED-GIA' target=\"_blank\">https://wandb.ai/uab-ai/MED-GIA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uab-ai/MED-GIA/runs/czvq7hz4' target=\"_blank\">https://wandb.ai/uab-ai/MED-GIA/runs/czvq7hz4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs:  8\n",
      "batch_size:  256\n",
      "learning_rate:  0.001\n",
      "k_folds:  5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HelicoDatasetClassification' object has no attribute 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 95\u001b[0m\n\u001b[0;32m     91\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m stratified_kfold \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39mk_folds, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(stratified_kfold\u001b[38;5;241m.\u001b[39msplit(train_dataset, \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m)):\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFOLD \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HelicoDatasetClassification' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from model import HelicobacterClassifier\n",
    "from utils import HelicoDatasetClassification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def train(model, loss_function, optimizer, train_loader, val_loader, device, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train the model on the given dataset for the specified number of epochs.\n",
    "\n",
    "    :param model: The model to train\n",
    "    :param loss_function: The loss function to use\n",
    "    :param optimizer: The optimizer to use\n",
    "    :param train_loader: The training data loader\n",
    "    :param val_loader: The validation data loader\n",
    "    :param num_epochs: The number of epochs to train for\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    print(\"Starting training\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, data in enumerate(train_loader): # Data is a tuple ([B, C, H, W], [B])\n",
    "            img, label = data\n",
    "            img = img.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(img)\n",
    "            loss = loss_function(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n",
    "        wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
    "        # Validation\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    img, label = data\n",
    "                    img = img.to(device)\n",
    "                    label = label.to(device)\n",
    "                    output = model(img)\n",
    "                    loss = loss_function(output, label)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(output.data, 1)\n",
    "                    total += label.size(0)\n",
    "                    correct += (predicted == label).sum().item()\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            accuracy = 100 * correct / total\n",
    "            print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss}, Accuracy: {accuracy}%\")\n",
    "            wandb.log({\"epoch\": epoch + 1, \"val_loss\": avg_val_loss, \"accuracy\": accuracy})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize wandb\n",
    "    wandb.login(key=\"07313fef21f9b32f2fb1fb00a2672258c8b5c3d4\")\n",
    "    wandb.init(project=\"MED-GIA\")\n",
    "    \n",
    "    # Set hyperparameters\n",
    "    wandb.config = {\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": 8,\n",
    "        \"batch_size\": 256,\n",
    "        \"optimizer\" : \"adam\",\n",
    "        \"k_folds\": 5\n",
    "    }\n",
    "\n",
    "    print(\"num_epochs: \", wandb.config[\"epochs\"])\n",
    "    print(\"batch_size: \", wandb.config[\"batch_size\"])\n",
    "    print(\"learning_rate: \", wandb.config[\"learning_rate\"])\n",
    "    print(\"k_folds: \", wandb.config[\"k_folds\"])\n",
    "    \n",
    "    # Load the dataset\n",
    "    dataset = HelicoDatasetClassification()\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    k_folds = wandb.config[\"k_folds\"]\n",
    "     \n",
    "    # Initialize the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    stratified_kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    train_labels = [label for _, label in train_dataset]\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(stratified_kfold.split(train_dataset, train_labels)):\n",
    "        print(f\"FOLD {fold}\")\n",
    "        print(\"--------------------------------\")\n",
    "        \n",
    "        # Sample elements randomly from a given list of indices, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "        \n",
    "        # Define data loaders for training and validation\n",
    "        train_loader = DataLoader(train_dataset, batch_size=wandb.config[\"batch_size\"], sampler=train_subsampler)\n",
    "        val_loader = DataLoader(train_dataset, batch_size=wandb.config[\"batch_size\"], sampler=val_subsampler)\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = HelicobacterClassifier()\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config[\"learning_rate\"])\n",
    "        \n",
    "        # Train the model\n",
    "        train(model, loss_function, optimizer, train_loader, val_loader, device, num_epochs=wandb.config[\"epochs\"])\n",
    "        \n",
    "        # Save the model for each fold\n",
    "        torch.save(model.state_dict(), f\"HelicobacterClassifier_fold{fold}.pth\")\n",
    "        wandb.save(f\"HelicobacterClassifier_fold{fold}.pth\")\n",
    "    \n",
    "    # Final training on the entire training dataset\n",
    "    final_train_loader = DataLoader(train_dataset, batch_size=wandb.config[\"batch_size\"], shuffle=True)\n",
    "    final_model = HelicobacterClassifier().to(device)\n",
    "    final_loss_function = nn.CrossEntropyLoss()\n",
    "    final_optimizer = torch.optim.Adam(final_model.parameters(), lr=wandb.config[\"learning_rate\"])\n",
    "    \n",
    "    train(final_model, final_loss_function, final_optimizer, final_train_loader, None, device, num_epochs=wandb.config[\"epochs\"])\n",
    "    \n",
    "    # Save the final model\n",
    "    torch.save(final_model.state_dict(), \"HelicobacterClassifier_final.pth\")\n",
    "    wandb.save(\"HelicobacterClassifier_final.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
